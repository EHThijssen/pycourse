{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## This is a short demonstration of some of the things\n",
    "## python can do for you, and a preview to some of the\n",
    "## things we'll learn in the course\n",
    "\n",
    "print('I\\'m ready for my life to be improved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Lets import some modules we'll need later!\n",
    "## This might take a minute. You can tell the cell is still\n",
    "## working because of the little [*] next to the 'ln' symbol \n",
    "## to the left.\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import math\n",
    "import pandas\n",
    "import nipype\n",
    "from glob import glob\n",
    "from scipy import stats,linalg\n",
    "from scipy.io import savemat\n",
    "import numpy as np\n",
    "import nibabel as ni\n",
    "import networkx as nx\n",
    "from matplotlib import pyplot,patches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################ PART 1: COMMAND LINE ###################\n",
    "## First, I'll show you a bit about how python can help you automate\n",
    "## command line functions\n",
    "\n",
    "# let's create a few empty files for demonstration purposes\n",
    "print('here is our directory before creating the files. . . \\n')\n",
    "print(os.listdir())\n",
    "for i in range(10):\n",
    "    os.system('touch empty_fl_%s'%i)\n",
    "print('\\n . . . and after')\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's say we want to rename these files given a list of subject IDs\n",
    "\n",
    "# generate a list of random subject IDs\n",
    "ids = np.random.randint(1000,5000,10)\n",
    "print('here is a list of the IDs we\\'ve generated')\n",
    "print(sorted(ids))\n",
    "print('\\n')\n",
    "\n",
    "# collect files that we created\n",
    "fls = sorted(glob(os.path.join(os.getcwd(),'empty_fl*')))\n",
    "print('here are the files')\n",
    "print(fls)\n",
    "print ('\\n')\n",
    "\n",
    "# rename files using subject IDs\n",
    "for i,fl in enumerate(fls):\n",
    "    pth,flnm = os.path.split(fl)\n",
    "    fl_parts = flnm.split('_')\n",
    "    os.system('mv %s %s'%(fl,os.path.join(pth,'%s_%s_s%s'%(fl_parts[0],fl_parts[1],ids[i]))))\n",
    "\n",
    "# now we'll list contents again to see if the files were renamed\n",
    "print('here are the directory contents')\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now lets get rid of those files.. we don't need them\n",
    "os.system('rm empty*')\n",
    "\n",
    "print('directory contents')\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Python can be used to automate any terminal commands\n",
    "## This includes neuroimaging tools like fsl. \n",
    "## Here, we will extract average effect (t) values for three \n",
    "## different subjects within a set of 7 masks taken from an \n",
    "## atlas of functional brain networks\n",
    "\n",
    "# collect subjects and atlas\n",
    "stuff_dir = os.path.join(pth,'stuff')\n",
    "subs = sorted(glob(os.path.join(stuff_dir,'rw*')))\n",
    "msk = os.path.join(stuff_dir,'networks_scale7.nii.gz')\n",
    "print('here is a list of our subjects')\n",
    "print(subs)\n",
    "print('\\n and our atlas')\n",
    "print(msk)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lets also make a dataframe to collect the values\n",
    "sub_ids = []\n",
    "for sub in subs:\n",
    "    sid = os.path.split(sub)[1].split('.')[0]\n",
    "    sub_ids.append(sid)\n",
    "\n",
    "cols = []\n",
    "for i in range(7):\n",
    "    cols.append('network_%s'%i)\n",
    " \n",
    "df = pandas.DataFrame(index=sub_ids,columns=cols)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now lets actually perform the command\n",
    "\n",
    "# go to the directory\n",
    "os.chdir(stuff_dir)\n",
    "\n",
    "# for each network\n",
    "for i in range(7):\n",
    "    # make mask of network using fslmaths\n",
    "    os.system('fslmaths %s -thr %s -uthr %s msk%s'%(msk,(i+1),(i+1),i)) \n",
    "    # for each subject\n",
    "    for x,sub in enumerate(subs):\n",
    "        # mask subject with mask using fslmaths\n",
    "        os.system('fslmaths %s -mas msk%s.nii.gz mskd_img'%(sub,i))\n",
    "        # get mean within mask by reading output from fslstats command\n",
    "        val = float(subprocess.check_output('fslstats mskd_img.nii.gz -M',shell=True))\n",
    "        # print value to keep track of progress\n",
    "        print('the mean value for subject %s in network %s is %s'%(sub_ids[x],i,val))\n",
    "        # write to DataFrame\n",
    "        df.ix[sub_ids[x],'network_%s'%(i)] = val\n",
    "# get rid of scrap\n",
    "os.system('rm msk*.ni*')\n",
    "\n",
    "print('\\n and here is our dataframe again with the values filled in \\n')\n",
    "print(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######### PART 2: SPREADSHEETS ############\n",
    "## Using pandas lets us open spreadsheets with ease\n",
    "\n",
    "# let's turn that dataframe into an actual file that can be read\n",
    "# what are our options?\n",
    "#df.to_\n",
    "\n",
    "df.to_excel('networks.xls')\n",
    "os.listdir()\n",
    "\n",
    "# we can also read in files of almost any text type using pandas\n",
    "ndf = pandas.ExcelFile('networks.xls').parse('Sheet1')\n",
    "\n",
    "print(ndf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# there are many nice features of pandas\n",
    "print('slicing is easy: \\n')\n",
    "\n",
    "print('all networks for subject %s'%(sub_ids[0]))\n",
    "print(ndf.ix[sub_ids[0]],'\\n')\n",
    "print('all subject values for network 0')\n",
    "print(ndf.ix[:,cols[0]],'\\n')\n",
    "\n",
    "print('so is indexing: \\n')\n",
    "print('value for subject %s, network 4'%(sub_ids[1]))\n",
    "print(ndf.ix[sub_ids[1],cols[4]],'\\n')\n",
    "\n",
    "print('there also many things you can do to summarize or manipulate individual slices \\n')\n",
    "slc = ndf.ix[:,cols[0]]\n",
    "print('here is the mean value across all networks for subject 0 \\n')\n",
    "print(slc.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "########## PART 3: MATH AND STATS ############\n",
    "## Python has an array of different libraries available for performing \n",
    "## mathematical operations. This ranges from simple mathematical commmands\n",
    "## to basic stats like regression/t-tests, to complex deep learning\n",
    "## algorithms such as clustering and support vector machines.\n",
    "\n",
    "# lets make z-scores for each network from the data we just produced\n",
    "\n",
    "# first we'll make a quick z-score function\n",
    "def z_score(val,mn,sd): \n",
    "    scr = (val - mn) / sd\n",
    "    return scr\n",
    "\n",
    "# next, we'll get the means and SDs of each network and store them\n",
    "# in a dictionary\n",
    "net_stz = {}\n",
    "for i in range(7):\n",
    "    slc = df[:]['network_%s'%(i)]\n",
    "    net_stz.update({'network_%s'%(i): (slc.mean(),slc.std())})\n",
    "print('here are the means and sds for each network \\n')\n",
    "print(net_stz,'\\n')\n",
    "    \n",
    "# now we'll apply it to each subject's data\n",
    "for sub in df.index.tolist():\n",
    "    for col in cols:\n",
    "        val = df.ix[sub,col]\n",
    "        df.ix[sub,'z_%s'%(col)] = z_score(val,net_stz[col][0],net_stz[col][1])\n",
    "\n",
    "print('here are the new z-scored values')\n",
    "print(df[:][df.columns[7:]])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## we can also calculate something slightly more complicated,\n",
    "## we'll make a fake correlation matrix from scratch, look for\n",
    "## networks using clustering analysis, and then calculate the \n",
    "## some graph theory metrics\n",
    "\n",
    "# generate random connectivity matrix\n",
    "gen_mtx = np.random.random((49,49))\n",
    "\n",
    "# now make its symettrical like a real connectivity matrix\n",
    "diag = gen_mtx.diagonal()\n",
    "lo_tri = np.tril(gen_mtx)\n",
    "up_tri = lo_tri.transpose()\n",
    "mtx = lo_tri + up_tri\n",
    "for i in range(49):\n",
    "    mtx[i][i] = diag[i]\n",
    "    \n",
    "print('here is our matrix \\n',mtx)\n",
    "print('\\n and here is what it looks like visualized')\n",
    "pyplot.imshow(mtx,\n",
    "              cmap=\"Greys\",\n",
    "              interpolation=\"none\")\n",
    "pyplot.show()\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# and lets make use partial correlations to make it slightly more sparse\n",
    "def partial_corr(C):\n",
    "    C = np.asarray(C)\n",
    "    p = C.shape[1]\n",
    "    P_corr = np.zeros((p, p), dtype=np.float)\n",
    "    for i in range(p):\n",
    "        P_corr[i, i] = 1\n",
    "        for j in range(i+1, p):\n",
    "            idx = np.ones(p, dtype=np.bool)\n",
    "            idx[i] = False\n",
    "            idx[j] = False\n",
    "            beta_i = linalg.lstsq(C[:, idx], C[:, j])[0]\n",
    "            beta_j = linalg.lstsq(C[:, idx], C[:, i])[0]\n",
    "\n",
    "            res_j = C[:, j] - C[:, idx].dot( beta_i)\n",
    "            res_i = C[:, i] - C[:, idx].dot(beta_j)\n",
    "            \n",
    "            corr = stats.pearsonr(res_i, res_j)[0]\n",
    "            P_corr[i, j] = corr\n",
    "            P_corr[j, i] = corr\n",
    "        \n",
    "    return P_corr\n",
    "\n",
    "pc_mtx = partial_corr(mtx)\n",
    "for i in range(49):\n",
    "    pc_mtx[i][i] = diag[0]\n",
    "\n",
    "print('now here\\'s the new matrix!')\n",
    "pyplot.imshow(pc_mtx,\n",
    "              cmap=\"Greys\",\n",
    "              interpolation=\"none\")\n",
    "pyplot.show()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's make some sense out of this nonsense. We'll run clustering\n",
    "# analysis on our random connectivity matrix\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "# run clustering analysis and get labels\n",
    "kmeans = KMeans(n_clusters=7, random_state=0).fit(pc_mtx)\n",
    "labz = kmeans.labels_.tolist()\n",
    "# reorder connectivity matrix by cluster labels\n",
    "naxis = []\n",
    "for x in range(7):\n",
    "    for i in range(49):\n",
    "        if labz[i] == x:\n",
    "            naxis.append(i)\n",
    "npc_mtx = pc_mtx[:, naxis][naxis]\n",
    "\n",
    "print('and here is our matrix reordered by cluster membership')\n",
    "pyplot.imshow(npc_mtx,\n",
    "              cmap=\"Greys\",\n",
    "              interpolation=\"none\")\n",
    "pyplot.show()\n",
    "print('wow, doesn\\'t it look like there is something real happening here?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## We could do the graph theory metrics by hand, but we don't\n",
    "## have to. There are specialized python functions that do the \n",
    "## dirty work for you for almost anything you could think of,\n",
    "## and graph theory is no exception\n",
    "\n",
    "## first lets convert this to a binarized matrix where we save\n",
    "## only the top 10% of connections\n",
    "\n",
    "flat = np.tril(pc_mtx).flatten()\n",
    "flat = [abs(x) for x in flat]\n",
    "top_no = int(len(flat) * 0.90)\n",
    "top_10 = sorted(flat)[top_no]\n",
    "bin_mtx = np.full((49,49),np.nan)\n",
    "for i in range(49):\n",
    "    for j in range(49):\n",
    "        if pc_mtx[i][j] > top_10:\n",
    "            bin_mtx[i][j] = 1\n",
    "        else:\n",
    "            bin_mtx[i][j] = 0\n",
    "nbin_mtx = bin_mtx[:, naxis][naxis]\n",
    "pyplot.imshow(nbin_mtx,\n",
    "              cmap=\"Greys\",\n",
    "              interpolation=\"none\")\n",
    "pyplot.show()\n",
    "print('as this is random data, the binarized matrix is not very pretty')\n",
    "\n",
    "# but assuming we had a real matrix it would look better\n",
    "# Lets run some graph metrix. Choose your poison\n",
    "\n",
    "G = nx.from_numpy_matrix(bin_mtx)\n",
    "nx.degree(G)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## we can also run more traditional statistics like correlations\n",
    "## lets generate some fake data and run a correlation\n",
    "\n",
    "# here's some fake data\n",
    "x = np.linspace(0, 1, 100)\n",
    "y = (np.cos(x) + 0.5*np.random.rand(100)) + np.random.ranf()\n",
    "#y = (x + np.random.rand(100)) - np.random.ranf()\n",
    "p = np.poly1d(np.polyfit(x, y, 1))\n",
    "t = np.linspace(0, 1, 100)\n",
    "pyplot.plot(x, y, 'o', t, p(t), '-')\n",
    "pyplot.show()\n",
    "\n",
    "# lets test it!\n",
    "r,pval = stats.pearsonr(x,y)\n",
    "print('r = %s, p = %s'%(r,pval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## That's all well and good, but with Python, we can add another level\n",
    "## of sophistication through automation: random sampling methods\n",
    "\n",
    "## Let's generate some fake data again.\n",
    "## But this time, lets make the correlation less tight\n",
    "## And to show off the versatility of python, let's use completely different\n",
    "## commands to generate the data and to plot\n",
    "\n",
    "# generate fake data\n",
    "xx = np.array([-1, 1])\n",
    "yy = np.array([-1.5, 1.5])\n",
    "means = [xx.mean(), yy.mean()]  \n",
    "stds = [xx.std() / 3, yy.std() / 3]\n",
    "corr = 0.3         # correlation\n",
    "covs = [[stds[0]**2          , stds[0]*stds[1]*corr], \n",
    "        [stds[0]*stds[1]*corr,           stds[1]**2]] \n",
    "\n",
    "# get plotting paramaters\n",
    "ms = np.random.multivariate_normal(means, covs, 50).T\n",
    "slp,inter = np.polyfit(ms[0],ms[1],1)\n",
    "\n",
    "# plot\n",
    "pyplot.plot(ms[0],ms[1], 'o')\n",
    "pyplot.plot(ms[0],slp*ms[0]+inter,'-')\n",
    "\n",
    "pyplot.show()\n",
    "\n",
    "# run stats\n",
    "r,pval = stats.pearsonr(ms[0],ms[1])\n",
    "print('r = %s, p = %s'%(r,pval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## let's say we wanted to resample the data to establish exact p-values\n",
    "## we can shuffle x (with replacement) over 10,000 permutations \n",
    "\n",
    "## let's also time it..\n",
    "import timeit\n",
    "\n",
    "distr = []\n",
    "start_time = timeit.default_timer()\n",
    "for i in range(10000):\n",
    "    new_x = np.random.permutation(ms[0].tolist())\n",
    "    new_r,new_p = stats.pearsonr(new_x,ms[1])\n",
    "    distr.append(new_r)\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "\n",
    "print('permuted %s samples in %s seconds \\n'%(len(distr),elapsed))# \n",
    "\n",
    "pyplot.hist(distr)\n",
    "pyplot.title(\"Null distribution of r-values\")\n",
    "pyplot.xlabel(\"r-value\")\n",
    "pyplot.ylabel(\"Frequency\")\n",
    "\n",
    "\n",
    "print('here is our distribution of r-values')\n",
    "pyplot.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now lets find our p-value and test the hypothesis that the\n",
    "# correlation did not occur from chance\n",
    "\n",
    "# assess null distribution to establish p-value\n",
    "slicer = []\n",
    "for v in sorted(distr):\n",
    "    if v > r:\n",
    "        slicer.append(v)\n",
    "exact_p = len(slicer)/float((len(distr)+1))\n",
    "\n",
    "\n",
    "# what r-value is p<0.05?\n",
    "sig_r = sorted(distr)[int((1-0.05) * len(distr))]\n",
    "# what r-value is p<0.025 (two tailed)\n",
    "sig_r2 = sorted(distr)[int((1-0.025) * len(distr))]\n",
    "\n",
    "# let's have a look\n",
    "print('the exact p-value is %s \\n'%(exact_p))\n",
    "print('the black line is the observed r-value')\n",
    "print('the dotted red line represents p<0.05 (one tailed)')\n",
    "print('the solid red line represents p<0.025 (two tailed)')\n",
    "\n",
    "pyplot.hist(distr,color='blue')\n",
    "pyplot.title(\"Null distribution of r-values\")\n",
    "pyplot.xlabel(\"r-value\")\n",
    "pyplot.ylabel(\"Frequency\")\n",
    "pyplot.axvline(x=sig_r,color='red',linestyle='dashed')\n",
    "pyplot.axvline(x=r,color='black')\n",
    "pyplot.axvline(x=sig_r2,color='red')\n",
    "pyplot.show()\n",
    "\n",
    "print(exact_p)\n",
    "print(sig_r)\n",
    "print(sig_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######## PART 4: Loading, saving and manipulating images ###########\n",
    "## None of this would be much good if you couldn't use it to deal with\n",
    "## actual imaging data. Luckily, python can handle that too.\n",
    "\n",
    "# let's take the first subject scan from earlier\n",
    "print('woring on subject %s'%(subs[0]))\n",
    "img = ni.load(subs[0])\n",
    "\n",
    "print('now we have access to the affine... \\n')\n",
    "print(img.affine)\n",
    "print('\\n and the header...\\n')\n",
    "print(img.header)\n",
    "print('\\n and the actual image data, which is a matrix of shape %s,%s,%s \\n'%(img.shape[0],\n",
    "                                                                                img.shape[1],\n",
    "                                                                                img.shape[2]))\n",
    "print(img.get_data())\n",
    "\n",
    "#print('\\n',img.get_data()[:][40][40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## You can perform analyses on these matrices just like you would any other\n",
    "## and can easily save the data back into a new images.\n",
    "## Just as a simple example, we'll threshold each image so that only top 5%\n",
    "## of non-zero values of each image shown, and then we'll binarize it to make \n",
    "## a mask.\n",
    "\n",
    "#and lets time it!\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "for sub in subs:\n",
    "    # get data and affine (for later)\n",
    "    s_img = ni.load(sub)\n",
    "    s_data = s_img.get_data()\n",
    "    s_aff = s_img.affine\n",
    "    sx,sy,sz = s_img.shape\n",
    "    # figure out 5% threshold for non-zero values\n",
    "    s_distr = []\n",
    "    for i in range(sx):\n",
    "        for j in range(sy):\n",
    "            for k in range(sz):\n",
    "                if s_data[i][j][k] > 1:\n",
    "                    s_distr.append(s_data[i][j][k])\n",
    "    thr = sorted(s_distr)[int(len(s_distr) * 0.95)]\n",
    "    print('value threshold for subject %s is %s'%(sub,thr))\n",
    "    # binarize image\n",
    "    s_bin = np.where(s_data>thr,1,0)\n",
    "    # save new binarized, thresholded image\n",
    "    pth,flnm = os.path.split(sub)\n",
    "    nflnm = os.path.join(pth,'%s_p05mask.nii.gz'%(flnm.split('.')[0]))\n",
    "    n_img = ni.Nifti1Image(s_bin, s_aff)\n",
    "    n_img.to_filename(nflnm)\n",
    "    print('image written to %s \\n'%(nflnm))\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print('\\n manipulation performed on 3 images in %s seconds'%(elapsed))\n",
    "print('assuming linear scaling, that would mean 300 subjects would take %s minutes'%((100*elapsed)/60))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Some poor souls hang nostalgically onto MatLab, believing\n",
    "## everything on earth is best represented my matrices, and that\n",
    "## all things should be followed by a ;\n",
    "## Luckily for those poor souls, we can easily export anything to a\n",
    "## .mat file that is compatible with matlab.\n",
    "\n",
    "# We still have our matrix from before. Let's make it into a matfile\n",
    "\n",
    "# First convert to a dictionary\n",
    "mat_dict = {'fake_matrix': pc_mtx}\n",
    "\n",
    "# Save the file\n",
    "savemat('new_matfile',mat_dict)\n",
    "\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nipype.interfaces.fsl as fsl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fsl.preprocess.FLIRT?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subs = glob(os.path.join(pth,'w*'))\n",
    "ref = '/Users/jakevogel/bellec_lab/aaic/rMNI152_T1_2mm_brain.nii'\n",
    "flt = fsl.preprocess.FLIRT()\n",
    "for sub in subs:\n",
    "    flt.inputs.in_file = sub\n",
    "    flt.inputs.reference = ref\n",
    "    flt.inputs.output_type = 'NIFTI_GZ'\n",
    "    flt.inputs.interp = 'nearestneighbour'\n",
    "    flt.cmdline\n",
    "    flt.run()\n",
    "#flt = fsl.FLIRT()\n",
    "#flt.inp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clean up\n",
    "os.system('rm *p05*')\n",
    "os.system('rm *flirt*')\n",
    "os.system('rm *.mat')\n",
    "os.system('rm *xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
